import numpy as np
import pandas as pd
from torch.utils.data import DataLoader
"""
Written based on the original data processing done by ACTINN 
to preserve compatibility with datasets processed by the TF version

"""


def type2label_dict(types):
    """
    Turn types into labels
    INPUT: 
        types-> types of cell present in the data
        
    RETURN
     celltype_to_label_dict-> type_to_label dictionary
    
    """

    all_celltype = list(set(types))
    celltype_to_label_dict = {}

    for i in range(len(all_celltype)):
        celltype_to_label_dict[all_celltype[i]] = i
    return celltype_to_label_dict


def convert_type2label(types, type_to_label_dict):
    """ 
    Convert types to labels
    INPUTS:
        types-> list of types
        type_to_label dictionary-> dictionary of cell types mapped to numerical labels
    
    RETURN: 
        labels-> list of labels
    
    """

    types = list(types)
    labels = list()
    for type in types:
        labels.append(type_to_label_dict[type])
    return labels


def scale_sets(sets, meta=None):
    """
    Get common genes, normalize  and scale the sets
    INPUTS: 
        sets-> a list of all the sets to be scaled
        meta-> some meta informations required
    
    RETURN: 
        sets-> normalized sets
    """
    eps = 1e-10
    common_genes = set(sets[0].index)
    if meta is not None:
        common_genes = set.intersection(set(meta["idx2sum"]), common_genes)
    else:
        for i in range(1, len(sets)):
            common_genes = set.intersection(set(sets[i].index), common_genes)
            print(f"common genes after {i} iteration: {len(common_genes)}")
    common_genes = sorted(list(common_genes))
    sep_point = [0]
    for i in range(len(sets)):
        sets[i] = sets[i].loc[common_genes, ]
        sep_point.append(sets[i].shape[1])

    total_set = pd.concat(sets, axis=1, sort=False)
    idx2sum = total_set.index
    idx2train = total_set.index

    total_set = np.array(total_set, dtype=np.float32)
    total_set = np.divide(total_set, np.sum(total_set, axis=0,
                                            keepdims=True)) * 20000
    total_set = np.log2(total_set + 1)

    if meta is not None:
        total_set = total_set[np.array(
            [idx in meta["idx2train"] for idx in idx2train], dtype=bool)]
    else:
        expr = np.sum(total_set, axis=1)
        idxer = np.logical_and(expr >= np.percentile(expr, 1),
                               expr <= np.percentile(expr, 99))
        total_set = total_set[idxer, ]
        idx2train = idx2train[idxer, ]

        cv = np.std(total_set, axis=1) / np.mean(total_set + eps, axis=1)
        idxer = np.logical_and(cv >= np.percentile(cv, 1),
                               cv <= np.percentile(cv, 99))
        total_set = total_set[idxer, ]
        idx2train = idx2train[idxer, ]

    for i in range(len(sets)):
        sets[i] = total_set[:,
                            sum(sep_point[:(i + 1)]):sum(sep_point[:(i + 2)])]
    idx2 = (idx2sum, idx2train)
    return sets, idx2


def CSV_IO(train_path: str,
           train_labels_path: str,
           test_path: str,
           test_labels_path: str,
           batchSize: int = 128,
           workers: int = 12,
           sum_index=None,
           train_index=None):
    """
    This function allows the use of data that was generated by the original ACTINN code (in TF)
    
    INPUTS
        train_path-> path to the h5 file for the training data (dataframe of Genes X Cells)
        train_labels_path-> path to the csv file of the training data labels (cell type strings)
        test_path-> path to the h5 file of the testing data (dataframe of Genes X Cells)
        test_labels_path-> path to the csv file of the test data labels (cell type strings)

    RETURN
        train_data_loader-> training data loader consisting of the data (at batch[0]) and labels (at batch[1])
        test_data_loader-> testing data loader consisting of the data (at batch[0]) and labels (at batch[1])
    
    """

    print("==> Reading in H5 Data frame (CSV)")
    test_set = pd.read_hdf(test_path, key="dge")
    idxer = np.unique(test_set.index, return_index=True)[1]
    test_set = test_set.iloc[idxer, ]
    print("removing duplicated")
    test_set = test_set.loc[test_set.sum(axis=1) > 0, :]
    print("removing singular")
    test_set.index = [s.upper() for s in test_set.index]
    test_set = test_set.loc[~test_set.index.duplicated(keep='first')]
    print("removing duplicated after upper")
    print(f"dimension of the matrix after removing them: {test_set.shape}")

    if train_index is None or sum_index is None:

        train_set = pd.read_hdf(train_path, key="dge")
        idxer = np.unique(train_set.index, return_index=True)[1]
        train_set = train_set.iloc[idxer, ]
        print("removing duplicated")
        train_set = train_set.loc[train_set.sum(axis=1) > 0, :]
        print("removing singular")
        train_set.index = [s.upper() for s in train_set.index]
        train_set = train_set.loc[~train_set.index.duplicated(keep='first')]
        print("removing duplicated after upper")
        print(
            f"dimension of the matrix after removing them: {train_set.shape}")

        train_label = pd.read_csv(train_labels_path, header=None, sep="\t")
        test_label = pd.read_csv(test_labels_path, header=None, sep="\t")

        sets, idx2 = scale_sets([train_set, test_set])
        train_set, test_set = sets
        type_to_label_dict = type2label_dict(train_label.iloc[:, 1])
        label_to_type_dict = {v: k for k, v in type_to_label_dict.items()}
        print(f"    -> Cell types in training set: {type_to_label_dict}")
        print(f"    -> # trainng cells: {train_label.shape[0]}")

        train_label = convert_type2label(train_label.iloc[:, 1],
                                         type_to_label_dict)
        test_label = convert_type2label(test_label.iloc[:, 1],
                                        type_to_label_dict)

        # we want to get Cells X Genes
        print(
            f"    *** Remember we the data is formatted as Cells X Genes ***")
        train_set = np.transpose(train_set)
        data_and_labels = []
        for i in range(len(train_set)):
            data_and_labels.append([train_set[i], train_label[i]])

        train_data_loader = DataLoader(data_and_labels,
                                       batch_size=batchSize,
                                       shuffle=True,
                                       sampler=None,
                                       batch_sampler=None,
                                       num_workers=workers,
                                       collate_fn=None,
                                       pin_memory=True)
    else:
        test_set = scale_sets([
            test_set,
        ], {
            "idx2sum": sum_index,
            "idx2train": train_index
        })[0][0]
        # have to return them after all
        idx2 = None
        label_to_type_dict = {}
        train_data_loader = None

    test_set = np.transpose(test_set)
    validation_data_and_labels = []
    for i in range(len(test_set)):
        validation_data_and_labels.append([test_set[i], test_label[i]])

    test_data_loader = DataLoader(validation_data_and_labels,
                                  batch_size=batchSize,
                                  shuffle=False,
                                  sampler=None,
                                  batch_sampler=None,
                                  num_workers=workers,
                                  collate_fn=None,
                                  pin_memory=True)

    return train_data_loader, test_data_loader, idx2, label_to_type_dict
